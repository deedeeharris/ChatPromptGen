import streamlit as st
import google.generativeai as genai
import os
from PIL import Image

# Function to initialize the Gemini model
def initialize_model(api_key):
    genai.configure(api_key=api_key)
    generation_config = {
        "temperature": 0.2,
        "top_p": 0.95,
        "top_k": 64,
        "max_output_tokens": 8192,
    }
    return genai.GenerativeModel(
        model_name="gemini-2.0-flash",
        generation_config=generation_config,
    )

# Function to translate text (using Gemini itself!)
def translate_text(model, text, target_language="en"):
    """Translates text using the Gemini model."""
    try:
        prompt = f"Translate the following text into {target_language}: {text}"
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        st.error(f"Translation error: {e}")  # Show error on Streamlit
        return None  # Return None instead of an empty string

# Function to create prompts and UI elements dynamically
def create_prompts_and_ui(model, user_language_code):
    """
    Generates all UI text and prompts dynamically using the Gemini model,
    including welcome messages, pillar explanations, and button text.  This
    avoids *any* hardcoded strings in the Python code, making the app
    fully AI-driven and easily translatable.
    """

    prompts = {}

    # --- Language-Specific Prompts ---
    # We define keys for each piece of text we need.  These keys are
    # language-agnostic.  The *values* will be generated by Gemini.

    prompt_keys = [
        "welcome_message",
        "role_prompt",
        "task_prompt",
        "context_prompt",
        "format_prompt",
        "examples_prompt",
        "generate_button",
        "change_anything_prompt",
        "updated_prompt_presentation",
        "here_is_the_system_prompt",
        "title",
        "subtitle",
        "settings_title",
        "api_key_input",
        "api_key_warning",
        "language_select",
    ]

    # --- Generate Prompts using Gemini ---
    for key in prompt_keys:
        try:
            # Construct a clear instruction for Gemini.
            generation_prompt = (
                f"Generate text for a Streamlit application in {user_language_code} "
                f"for the following purpose: '{key}'.  "
                f"Provide ONLY the text, with no extra explanation or formatting. "
                f"Keep it concise and suitable for UI display."
            )

            # Special instructions to improve output quality and conciseness.
            if key == "welcome_message":
                 generation_prompt += (
                    " Introduce yourself as an expert AI System Prompt Architect. "
                    "State the purpose (to collaboratively design a system prompt). "
                    "Be enthusiastic and inviting."
                 )
            elif "prompt" in key and key != "welcome_message" and key!= "change_anything_prompt" and key != "here_is_the_system_prompt" and key != "updated_prompt_presentation":
                generation_prompt += (
                    " This is a question to the user. Phrase it clearly and concisely. "
                    "Encourage detailed responses."
                )
            elif key == "generate_button":
                generation_prompt += " This is text for a button."
            elif key == "change_anything_prompt":
                generation_prompt += "Ask the user if they want to make any changes."
            elif key == "updated_prompt_presentation":
                generation_prompt += "State that this is the updated system prompt."
            elif key == "here_is_the_system_prompt":
                generation_prompt += "State that this is the generated system prompt."
            elif key == "title":
                generation_prompt += "This is for the App Title"
            elif key == "subtitle":
                generation_prompt += "This is for the App Subtitle"
            elif key == "settings_title":
                generation_prompt += "This is for the settings label."
            elif key == "api_key_input":
                generation_prompt += "Instruction for api key input."
            elif key == "api_key_warning":
                generation_prompt += "Warning message if api key isn't entered."
            elif key == "language_select":
                generation_prompt += "Instruction for selecting the language."

            response = model.generate_content(generation_prompt)

            # Store generated text using the key.
            prompts[key] = response.text.strip()

        except Exception as e:
            st.error(f"Error generating prompt for '{key}': {e}")
            prompts[key] = f"Error: Could not generate text for {key}"  # Fallback

    return prompts



# Function to create system prompt (now retrieves from secrets)
def create_system_prompt(model, user_language_code, user_input_role, user_input_task, user_input_context, user_input_format, user_input_examples, prompts):
    """Creates the system prompt based on user input."""

    base_system_prompt_template = st.secrets["base_system_prompt_template"]
    translated_system_prompt_template = translate_text(model, base_system_prompt_template, user_language_code)

    if translated_system_prompt_template is None:  # Handle translation failure
        return None, None, None, None, None, None, None

    final_system_prompt = translated_system_prompt_template.replace("[ROLE]", user_input_role, 1).replace("[TASK]", user_input_task, 1).replace("[CONTEXT]", user_input_context, 1).replace("[FORMAT]", user_input_format, 1)
    return final_system_prompt, prompts["role_prompt"], prompts["task_prompt"], prompts["context_prompt"], prompts["format_prompt"], prompts["examples_prompt"], prompts["welcome_message"]

# --- Main Streamlit App ---
def main():
    st.set_page_config(page_title="AI System Prompt Architect", page_icon="✨", layout="wide")

    # --- Sidebar for API Key and Language Selection ---
    with st.sidebar:
        # We can't *completely* avoid hardcoding *something*, so we hardcode
        # the language names and codes.  Everything else will be AI-generated.
        language_names = ["English", "Español", "Français", "한국어", "日本語", "中文", "हिन्दी"]
        language_codes = {
            "English": "en", "Español": "es", "Français": "fr",
            "한국어": "ko", "日本語": "ja", "中文": "zh", "हिन्दी": "hi"
        }

        # Get user's language choice.  Default to English.
        user_language = st.selectbox("Select your language:", language_names, index=0)
        user_language_code = language_codes[user_language]

        # Initialize model here, so it's available for prompt generation.
        api_key = st.text_input("Enter your Gemini API Key:", type="password")
        if not api_key:
            st.warning("Please enter your Gemini API key.")
            st.stop() # Stop execution if no API key

        model = initialize_model(api_key)
        prompts = create_prompts_and_ui(model, user_language_code)

        st.title(prompts["settings_title"]) # Use AI generated title.
        st.write(prompts["api_key_input"])
        # if not api_key:
        #    st.warning(prompts["api_key_warning"])
        #    st.stop()

        # user_language = st.selectbox(prompts["language_select"], language_names, index=0)
        # user_language_code = language_codes[user_language]

    # --- Main UI ---
    # Use AI-generated text for title and subtitle.
    st.markdown(f"<h1 class='title'>{prompts['title']}</h1>", unsafe_allow_html=True)
    st.markdown(f"<p class='subtitle'>{prompts['subtitle']}</p>", unsafe_allow_html=True)

    # Initialize session state.
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "current_stage" not in st.session_state:
        st.session_state.current_stage = "welcome"
    if "prompt_data" not in st.session_state:
        st.session_state.prompt_data = {}

    # --- Chat History Display ---
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # --- Input Handling and Prompt Generation ---

    if st.session_state.current_stage == "welcome":
        final_system_prompt, translated_role_prompt, translated_task_prompt, translated_context_prompt, translated_format_prompt, translated_examples_prompt, welcome_message_translated = create_system_prompt(model, user_language_code, "", "", "", "","", prompts)
        st.session_state.chat_history.append({"role": "assistant", "content": welcome_message_translated})
        with st.chat_message("assistant"):
                st.markdown(welcome_message_translated)
        st.session_state.current_stage = "role"

    elif st.session_state.current_stage == "role":
        if prompt := st.chat_input(translated_role_prompt):
            st.session_state.chat_history.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            st.session_state.prompt_data['role'] = prompt
            st.session_state.current_stage = "task"

    elif st.session_state.current_stage == "task":
        if prompt := st.chat_input(translated_task_prompt):
            st.session_state.chat_history.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            st.session_state.prompt_data['task'] = prompt
            st.session_state.current_stage = "context"

    elif st.session_state.current_stage == "context":
        if prompt := st.chat_input(translated_context_prompt):
            st.session_state.chat_history.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            st.session_state.prompt_data['context'] = prompt
            st.session_state.current_stage = "format"

    elif st.session_state.current_stage == "format":
        if prompt := st.chat_input(translated_format_prompt):
            st.session_state.chat_history.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            st.session_state.prompt_data['format'] = prompt
            st.session_state.current_stage = "examples"

    elif st.session_state.current_stage == "examples":
        if prompt := st.chat_input(translated_examples_prompt):
            st.session_state.chat_history.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            st.session_state.prompt_data['examples'] = prompt
            st.session_state.current_stage = "generate"
            final_system_prompt, _, _, _, _, _, _ = create_system_prompt(
                model,
                user_language_code,
                st.session_state.prompt_data.get('role', ''),
                st.session_state.prompt_data.get('task', ''),
                st.session_state.prompt_data.get('context', ''),
                st.session_state.prompt_data.get('format', ''),
                st.session_state.prompt_data.get('examples', ''),
                prompts
            )
            st.session_state.final_prompt = final_system_prompt
            translated_prompt_presentation = prompts["here_is_the_system_prompt"]
            st.session_state.chat_history.append({"role": "assistant", "content": translated_prompt_presentation + "\n\n" + final_system_prompt})
            with st.chat_message("assistant"):
                st.markdown(translated_prompt_presentation + "\n\n" + final_system_prompt)

    elif st.session_state.current_stage == "generate":
        if prompt := st.chat_input(prompts["change_anything_prompt"]):
            st.session_state.chat_history.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            final_system_prompt, _, _, _, _, _, _ = create_system_prompt(
                model,
                user_language_code,
                st.session_state.prompt_data.get('role', ''),
                st.session_state.prompt_data.get('task', ''),
                st.session_state.prompt_data.get('context', ''),
                st.session_state.prompt_data.get('format', ''),
                st.session_state.prompt_data.get('examples', '') + " additional notes: " + prompt,
                prompts
            )
            st.session_state.final_prompt = final_system_prompt
            translated_prompt_presentation = prompts["updated_prompt_presentation"]
            st.session_state.chat_history.append({"role": "assistant", "content": translated_prompt_presentation + "\n\n" + final_system_prompt})
            with st.chat_message("assistant"):
                st.markdown(translated_prompt_presentation + "\n\n" + final_system_prompt)

if __name__ == "__main__":
    main()
